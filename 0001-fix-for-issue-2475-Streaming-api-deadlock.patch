From 4ec5b7636318d984c283d42901389e042166bb70 Mon Sep 17 00:00:00 2001
From: Chris Rude <chrisrude@users.noreply.github.com>
Date: Sat, 8 Jul 2023 00:14:44 -0700
Subject: [PATCH] fix for issue #2475: Streaming api deadlock

The streaming API runs on its own thread.  This single thread can
handle multiple simultaneous requests from different websockets using
asyncio.

Currently, when multiple requests are received at once, we try to
acquire the shared lock shared.generation_lock multiple times in
succession in the same thread, which will cause a deadlock.

To avoid this, we create a decorator which will grab a thread-local
asyncio.Lock before we try to acquire the shared lock.  This will
prevent the deadlock.

This is a little more complicated than it needs to be because
currently there's only one thread, so we could just use a single,
global asyncio lock instead of one in TLS.  However this approach
will scale in the future if we ever want to have multiple threads
handling requests in parallel.
---
 extensions/api/streaming_api.py | 164 +++++++++++++++++++++-----------
 1 file changed, 109 insertions(+), 55 deletions(-)

diff --git a/extensions/api/streaming_api.py b/extensions/api/streaming_api.py
index 2a7fb62..cd1fe6d 100644
--- a/extensions/api/streaming_api.py
+++ b/extensions/api/streaming_api.py
@@ -1,5 +1,7 @@
 import asyncio
+import functools
 import json
+import threading
 from threading import Thread
 
 from websockets.server import serve
@@ -12,72 +14,124 @@ from modules.text_generation import generate_reply
 PATH = '/api/v1/stream'
 
 
-async def _handle_connection(websocket, path):
+# We use a thread local to store the asyncio lock, so that each thread
+# has its own lock.  This isn't strictly necessary, but it makes it
+# such that if we can support multiple worker threads in the future,
+# thus handling multiple requests in parallel.
+streaming_tls = threading.local()
+
+
+def get_api_lock(tls) -> asyncio.Lock:
+    """
+    The streaming and blocking API implementations each run on their own
+    thread, and multiplex requests using asyncio.  If multiple outstanding
+    requests are received at once, we will try to acquire the shared lock
+    shared.generation_lock multiple times in succession in the same thread,
+    which will cause a deadlock.
 
-    if path == '/api/v1/stream':
-        async for message in websocket:
-            message = json.loads(message)
+    To avoid this, we use this wrapper function to block on an asyncio
+    lock, and then try and grab the shared lock only while holding
+    the asyncio lock.
+    """
+    if not hasattr(tls, "asyncio_lock"):
+        tls.asyncio_lock = asyncio.Lock()
+
+    return tls.asyncio_lock
+
+
+def with_api_lock(tls):
+    """
+    This decorator should be added to all streaming API methods which
+    require access to the shared.generation_lock.  It ensures that the
+    tls.asyncio_lock is acquired before the method is called, and
+    released afterwards.
+    """
+    def api_lock_decorator(func):
+        @functools.wraps(func)
+        async def api_wrapper(*args, **kwargs):
+            async with get_api_lock(tls):
+                return await func(*args, **kwargs)
+        return api_wrapper
+
+    return api_lock_decorator
+
+
+@with_api_lock(streaming_tls)
+async def handle_stream_message(websocket, message):
+    message = json.loads(message)
+
+    prompt = message['prompt']
+    generate_params = build_parameters(message)
+    stopping_strings = generate_params.pop('stopping_strings')
+    generate_params['stream'] = True
 
-            prompt = message['prompt']
-            generate_params = build_parameters(message)
-            stopping_strings = generate_params.pop('stopping_strings')
-            generate_params['stream'] = True
+    generator = generate_reply(
+        prompt, generate_params, stopping_strings=stopping_strings, is_chat=False)
 
-            generator = generate_reply(
-                prompt, generate_params, stopping_strings=stopping_strings, is_chat=False)
+    # As we stream, only send the new bytes.
+    skip_index = 0
+    message_num = 0
 
-            # As we stream, only send the new bytes.
-            skip_index = 0
-            message_num = 0
+    for a in generator:
+        to_send = a[skip_index:]
+        if to_send is None or chr(0xfffd) in to_send:  # partial unicode character, don't send it yet.
+            continue
 
-            for a in generator:
-                to_send = a[skip_index:]
-                if to_send is None or chr(0xfffd) in to_send:  # partial unicode character, don't send it yet.
-                    continue
+        await websocket.send(json.dumps({
+            'event': 'text_stream',
+            'message_num': message_num,
+            'text': to_send
+        }))
 
-                await websocket.send(json.dumps({
-                    'event': 'text_stream',
-                    'message_num': message_num,
-                    'text': to_send
-                }))
+        await asyncio.sleep(0)
+        skip_index += len(to_send)
+        message_num += 1
 
-                await asyncio.sleep(0)
-                skip_index += len(to_send)
-                message_num += 1
+    await websocket.send(json.dumps({
+        'event': 'stream_end',
+        'message_num': message_num
+    }))
 
-            await websocket.send(json.dumps({
-                'event': 'stream_end',
-                'message_num': message_num
-            }))
+
+@with_api_lock(streaming_tls)
+async def handle_chat_stream_message(websocket, message):
+    body = json.loads(message)
+
+    user_input = body['user_input']
+    generate_params = build_parameters(body, chat=True)
+    generate_params['stream'] = True
+    regenerate = body.get('regenerate', False)
+    _continue = body.get('_continue', False)
+
+    generator = generate_chat_reply(
+        user_input, generate_params, regenerate=regenerate, _continue=_continue, loading_message=False)
+
+    message_num = 0
+    for a in generator:
+        await websocket.send(json.dumps({
+            'event': 'text_stream',
+            'message_num': message_num,
+            'history': a
+        }))
+
+        await asyncio.sleep(0)
+        message_num += 1
+
+    await websocket.send(json.dumps({
+        'event': 'stream_end',
+        'message_num': message_num
+    }))
+
+
+async def _handle_connection(websocket, path):
+
+    if path == '/api/v1/stream':
+        async for message in websocket:
+            await handle_stream_message(websocket, message)
 
     elif path == '/api/v1/chat-stream':
         async for message in websocket:
-            body = json.loads(message)
-
-            user_input = body['user_input']
-            generate_params = build_parameters(body, chat=True)
-            generate_params['stream'] = True
-            regenerate = body.get('regenerate', False)
-            _continue = body.get('_continue', False)
-
-            generator = generate_chat_reply(
-                user_input, generate_params, regenerate=regenerate, _continue=_continue, loading_message=False)
-
-            message_num = 0
-            for a in generator:
-                await websocket.send(json.dumps({
-                    'event': 'text_stream',
-                    'message_num': message_num,
-                    'history': a
-                }))
-
-                await asyncio.sleep(0)
-                message_num += 1
-
-            await websocket.send(json.dumps({
-                'event': 'stream_end',
-                'message_num': message_num
-            }))
+            await handle_chat_stream_message(websocket, message)
 
     else:
         print(f'Streaming api: unknown path: {path}')
-- 
2.34.1

